{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b47b76f-5989-4d16-bd37-645cf0aded5b",
   "metadata": {},
   "source": [
    "# Temporal Firing Rate Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8913a6-120f-41b9-8118-e516a62d6b96",
   "metadata": {},
   "source": [
    "##### Reads from many sheets (files), measures per neuron, per bin and per network with frequency domain inlcuded\n",
    "- Gets the excel out put of neuroexplorer (ratehisto output of nex file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d647482-0ced-4d68-89d0-1352e6f39857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = r'C:\\FiringFreqHisto'\n",
    "output_file = r'C:\\FiringFreqHisto_02.xlsx'\n",
    "\n",
    "sheet1_data = {}\n",
    "sheet2_data = {'Bin': None}\n",
    "sheet3_data = {'Frequency (Hz)': list(range(101))}\n",
    "sheet4_data = {'Frequency (Hz)': list(range(101))}\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.xlsx'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        excel_file = pd.ExcelFile(file_path)\n",
    "        for sheet_name in excel_file.sheet_names:\n",
    "            df = excel_file.parse(sheet_name)\n",
    "            if 'nr_001' in df.columns:\n",
    "                df = df.drop(columns=['nr_001'])\n",
    "            bins = df['Bin Left']\n",
    "            neuron_data = df.iloc[:, 2:]  \n",
    "            if neuron_data.empty:\n",
    "                continue\n",
    "            neuron_averages = neuron_data.mean(axis=0)\n",
    "            sheet1_data[f'{file_name}_{sheet_name}'] = neuron_averages\n",
    "            bin_averages = neuron_data.mean(axis=1)\n",
    "            if sheet2_data['Bin'] is None:\n",
    "                sheet2_data['Bin'] = bins \n",
    "            sheet2_data[f'{file_name}_{sheet_name}'] = bin_averages.values\n",
    "            freq_counts = neuron_data.stack().value_counts(bins=range(101), sort=False)\n",
    "            freq_counts.index = freq_counts.index.left\n",
    "            freq_counts = freq_counts.reindex(range(101), fill_value=0)\n",
    "            sheet3_data[f'{file_name}_{sheet_name}'] = freq_counts.values\n",
    "            total_events = freq_counts.sum()\n",
    "            freq_percentages = freq_counts / total_events * 100\n",
    "            sheet4_data[f'{file_name}_{sheet_name}'] = freq_percentages.values\n",
    "sheet1_df = pd.DataFrame(sheet1_data)\n",
    "if sheet2_data['Bin'] is None:\n",
    "    raise ValueError(\"No valid data was found in any of the sheets.\")\n",
    "sheet2_df = pd.DataFrame(sheet2_data)\n",
    "sheet3_df = pd.DataFrame(sheet3_data)\n",
    "sheet4_df = pd.DataFrame(sheet4_data)\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    sheet1_df.to_excel(writer, sheet_name='Neuron Averages', index=False)\n",
    "    sheet2_df.to_excel(writer, sheet_name='Bin Averages', index=False)\n",
    "    sheet3_df.to_excel(writer, sheet_name='Frequency Counts', index=False)\n",
    "    sheet4_df.to_excel(writer, sheet_name='Frequency Percentages', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1378492-c52a-4e25-9891-274af2f57823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "folder_path = r'C:\\FiringFreqHisto_02'\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('Baseline.xlsx'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        excel_file = pd.ExcelFile(file_path)\n",
    "        for sheet_name in excel_file.sheet_names:\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "            if 'nr_001' in df.columns:\n",
    "                df = df.drop(columns=['nr_001'])\n",
    "            bins = df['Bin Left'].values\n",
    "            neuron_data = df.iloc[:, 2:].values\n",
    "            bin_avg = np.mean(neuron_data, axis=1)\n",
    "            plt.figure(figsize=(14, 12))  # Adjusted size to accommodate additional plots\n",
    "            ax = sns.heatmap(neuron_data / neuron_data.max(axis=0), cmap='viridis', cbar=False) \n",
    "            cbar = plt.colorbar(ax.collections[0], ax=ax, orientation='vertical', pad=0.02)\n",
    "            cbar.set_label('Normalized Firing Frequency')\n",
    "            plt.suptitle(sheet_name, fontsize=14, fontweight='bold', y=1.02)  # Adjusted title position\n",
    "            ax_avg_neurons = ax.figure.add_axes([ax.get_position().x0, ax.get_position().y1 + 0.15, ax.get_position().width, 0.06])\n",
    "            neuron_ids = np.arange(neuron_data.shape[1])\n",
    "            firing_rates = neuron_data.mean(axis=0)  \n",
    "            ax_avg_neurons.scatter(neuron_ids, firing_rates, color='black', s=20)  \n",
    "            ax_avg_neurons.set_xlim(0, len(neuron_ids) - 1)  \n",
    "            ax_avg_neurons.set_xticks([]) \n",
    "            ax_avg_neurons.set_yticks([firing_rates.min(), firing_rates.max()])  \n",
    "            ax_avg_neurons.set_ylabel('Firing Rate (Hz)')\n",
    "            ax_avg_neurons.yaxis.set_ticks_position('left')\n",
    "            ax_avg_bins = ax.figure.add_axes([ax.get_position().x1 + 0.02, ax.get_position().y0, 0.1, ax.get_position().height])\n",
    "            ax_avg_bins.plot(bin_avg, np.arange(len(bin_avg)), color='black')\n",
    "            ax_avg_bins.invert_yaxis()\n",
    "            ax_avg_bins.set_ylim(len(bin_avg) - 1, 0)  # Inverted y-axis with min/max bins\n",
    "            ax_avg_bins.set_xticks([bin_avg.min(), bin_avg.max()])  # Show min and max on x-axis\n",
    "            ax_avg_bins.set_yticks([])  # Hide y ticks\n",
    "            ax_avg_bins.set_xticklabels([f'{bin_avg.min():.2f}', f'{bin_avg.max():.2f}'])  # Label min and max x-axis\n",
    "            #output_path = os.path.join(folder_path, f\"{sheet_name}.pdf\")\n",
    "            #plt.savefig(output_path, format='pdf', bbox_inches='tight')  # Ensure everything fits within the PDF\n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bd577b-e48b-4cb8-84e5-dad008a4d881",
   "metadata": {},
   "source": [
    "## measure synchronicity based on Bin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daad1e8-f378-4490-90cd-bfd626d1673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import correlate\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "folder_path = r'C:\\FiringFreqHisto_02'\n",
    "synchronicity_results = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('Baseline.xlsx'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        excel_file = pd.ExcelFile(file_path)\n",
    "        for sheet_name in excel_file.sheet_names:\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "            if 'nr_001' in df.columns:\n",
    "                df = df.drop(columns=['nr_001'])\n",
    "            neuron_data = df.iloc[:, 2:].values\n",
    "            neuron_maxes = neuron_data.max(axis=0)\n",
    "            neuron_maxes[neuron_maxes == 0] = 1 \n",
    "            normalized_data = neuron_data / neuron_maxes\n",
    "            normalized_data = np.nan_to_num(normalized_data, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            num_neurons = normalized_data.shape[1]\n",
    "            correlation_matrix = np.corrcoef(normalized_data.T)\n",
    "            mean_correlation = np.mean(correlation_matrix[np.triu_indices_from(correlation_matrix, k=1)])\n",
    "            mean_scc = np.mean([pearsonr(normalized_data[:, i], normalized_data[:, j])[0]\n",
    "                                for i in range(num_neurons)\n",
    "                                for j in range(i + 1, num_neurons)\n",
    "                                if not np.any(np.isnan(normalized_data[:, i])) and not np.any(np.isnan(normalized_data[:, j]))])\n",
    "            cross_corrs = [np.max(correlate(normalized_data[:, i], normalized_data[:, j], mode='valid'))\n",
    "                           for i in range(num_neurons)\n",
    "                           for j in range(i + 1, num_neurons)]\n",
    "            mean_cross_corr = np.mean(cross_corrs)\n",
    "            distances = pdist(normalized_data.T, metric='euclidean')\n",
    "            mutual_information = np.mean(distances)\n",
    "            synchronicity_results.append({\n",
    "                'Source File': filename,\n",
    "                'Sheet Name': sheet_name,\n",
    "                'Mean Correlation': mean_correlation,\n",
    "                'Mean SCC': mean_scc,\n",
    "                'Mean Cross-Correlation': mean_cross_corr,\n",
    "                'Mutual Information': mutual_information\n",
    "            })\n",
    "\n",
    "synchronicity_df = pd.DataFrame(synchronicity_results)\n",
    "output_file = os.path.join(folder_path, 'RateHisto_Synchronicity_Metrics.xlsx')\n",
    "synchronicity_df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325ece58-72dc-494a-874d-6ac03f84b753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import correlate\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "folder_path = r'C:\\FiringFreqHisto_02'\n",
    "synchronicity_results = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('Baseline.xlsx'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        excel_file = pd.ExcelFile(file_path)\n",
    "        for sheet_name in excel_file.sheet_names:\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "            if 'nr_001' in df.columns:\n",
    "                df = df.drop(columns=['nr_001'])\n",
    "            neuron_data = df.iloc[:, 2:].values\n",
    "            neuron_maxes = neuron_data.max(axis=0)\n",
    "            neuron_maxes[neuron_maxes == 0] = 1  \n",
    "            normalized_data = neuron_data / neuron_maxes\n",
    "            normalized_data = np.nan_to_num(normalized_data, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            num_neurons = normalized_data.shape[1]\n",
    "            correlation_matrix = np.corrcoef(normalized_data.T)\n",
    "            mean_correlation = np.mean(correlation_matrix[np.triu_indices_from(correlation_matrix, k=1)])\n",
    "            mean_scc = np.mean([pearsonr(normalized_data[:, i], normalized_data[:, j])[0]\n",
    "                                for i in range(num_neurons)\n",
    "                                for j in range(i + 1, num_neurons)\n",
    "                                if not np.any(np.isnan(normalized_data[:, i])) and not np.any(np.isnan(normalized_data[:, j]))])\n",
    "            cross_corrs = [np.max(correlate(normalized_data[:, i], normalized_data[:, j], mode='valid'))\n",
    "                           for i in range(num_neurons)\n",
    "                           for j in range(i + 1, num_neurons)]\n",
    "            mean_cross_corr = np.mean(cross_corrs)\n",
    "            distances = pdist(normalized_data.T, metric='euclidean')\n",
    "            mutual_information = np.mean(distances)\n",
    "            pairwise_correlations = [pearsonr(normalized_data[:, i], normalized_data[:, j])[0]\n",
    "                                     for i in range(num_neurons)\n",
    "                                     for j in range(i + 1, num_neurons)]\n",
    "            mean_sync_index = np.mean(pairwise_correlations)\n",
    "            synchronicity_results.append({\n",
    "                'Source File': filename,\n",
    "                'Sheet Name': sheet_name,\n",
    "                'Mean Correlation': mean_correlation,\n",
    "                'Mean SCC': mean_scc,\n",
    "                'Mean Cross-Correlation': mean_cross_corr,\n",
    "                'Mutual Information': mutual_information,\n",
    "                'Synchrony Index': mean_sync_index,\n",
    "                'Number of Neurons': num_neurons\n",
    "            })\n",
    "\n",
    "synchronicity_df = pd.DataFrame(synchronicity_results)\n",
    "output_file = os.path.join(folder_path, 'RateHisto_Synchronicity_Metrics.xlsx')\n",
    "synchronicity_df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9951c87d-3c20-495d-b8ec-ca6c9b567aaf",
   "metadata": {},
   "source": [
    "RateHisto_Organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee39413-f92f-4bc0-9988-b48d38a3d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r'C:\\RateHisto_02.xlsx'\n",
    "xls = pd.ExcelFile(file_path)\n",
    "\n",
    "sheets_to_process = ['Frequency Percentages', 'Frequency Counts']\n",
    "\n",
    "processed_dfs = []\n",
    "for sheet_name in sheets_to_process:\n",
    "    df = pd.read_excel(xls, sheet_name)\n",
    "    frequency_values = df['Frequency (Hz)']\n",
    "    data = []\n",
    "    for col in df.columns[1:]:\n",
    "        sample_name = col\n",
    "        group = 'Vasc' if 'VASCU' in sample_name else 'NonVasc'\n",
    "        week = 'w17' if 'w17' in sample_name else 'w20' if 'w20' in sample_name else 'w23'\n",
    "        sample_values = df[col].values\n",
    "        row = [group, week, sample_name] + sample_values.tolist()\n",
    "        data.append(row)\n",
    "    columns = ['Group', 'Week', 'Sample Name'] + frequency_values.tolist()\n",
    "    processed_df = pd.DataFrame(data, columns=columns)\n",
    "    processed_dfs.append((sheet_name, processed_df))\n",
    "output_file_path = r'C:\\RateHisto_Organized.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_file_path, engine='xlsxwriter') as writer:\n",
    "    for sheet_name, processed_df in processed_dfs:\n",
    "        processed_df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3727c49-fd20-462f-b80e-cd8467a86876",
   "metadata": {},
   "source": [
    "## Plot Percentage per Freq Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6530b066-206f-41c9-9186-a2a43ea747a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_path = r'C:\\RateHisto_Organized.xlsx'\n",
    "df = pd.read_excel(file_path, sheet_name='Frequency Percentages')\n",
    "\n",
    "df_melted = df.melt(id_vars=['Group', 'Week', 'Sample Name'], var_name='Frequency (Hz)', value_name='Percentage')\n",
    "df_melted['Frequency (Hz)'] = pd.to_numeric(df_melted['Frequency (Hz)'])\n",
    "\n",
    "grouped = df_melted.groupby(['Group', 'Week', 'Frequency (Hz)']).agg(\n",
    "    mean_percentage=('Percentage', 'mean'),\n",
    "    sem_percentage=('Percentage', 'sem')\n",
    ").reset_index()\n",
    "\n",
    "weeks = grouped['Week'].unique()\n",
    "fig, axes = plt.subplots(1, len(weeks), figsize=(20, 6), sharey=True)\n",
    "\n",
    "for i, week in enumerate(weeks):\n",
    "    ax = axes[i]\n",
    "    sns.lineplot(\n",
    "        data=grouped[grouped['Week'] == week], \n",
    "        x='Frequency (Hz)', \n",
    "        y='mean_percentage', \n",
    "        hue='Group', \n",
    "        style='Group', \n",
    "        markers=True, \n",
    "        errorbar='se',  # Updated errorbar parameter\n",
    "        ax=ax, \n",
    "        palette='Set1'\n",
    "    )\n",
    "    ax.set_title(f'Week {week}')\n",
    "    ax.set_xlabel('Frequency (Hz)')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Average Percentage')\n",
    "    else:\n",
    "        ax.set_ylabel('')\n",
    "    ax.legend(title='Group')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "grouped_all_weeks = df_melted.groupby(['Group', 'Frequency (Hz)']).agg(\n",
    "    mean_percentage=('Percentage', 'mean'),\n",
    "    sem_percentage=('Percentage', 'sem')\n",
    ").reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    data=grouped_all_weeks, \n",
    "    x='Frequency (Hz)', \n",
    "    y='mean_percentage', \n",
    "    hue='Group', \n",
    "    style='Group', \n",
    "    markers=True, \n",
    "    errorbar='se',  \n",
    "    palette='Set1'\n",
    ")\n",
    "plt.title('Mean Percentage Across All Weeks')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Average Percentage')\n",
    "plt.legend(title='Group')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0747bcd-0472-411e-bd2b-78cac7b9b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Excel file and read the Frequency Percentages sheet\n",
    "file_path = r'C:\\RateHisto_Organized.xlsx'\n",
    "df = pd.read_excel(file_path, sheet_name='Frequency Percentages')\n",
    "df_melted = df.melt(id_vars=['Group', 'Week', 'Sample Name'], var_name='Frequency (Hz)', value_name='Percentage')\n",
    "df_melted['Frequency (Hz)'] = pd.to_numeric(df_melted['Frequency (Hz)'])\n",
    "df_melted = df_melted[(df_melted['Frequency (Hz)'] != 0) & (df_melted['Frequency (Hz)'] != 100)]\n",
    "grouped = df_melted.groupby(['Group', 'Week', 'Frequency (Hz)']).agg(\n",
    "    mean_percentage=('Percentage', 'mean'),\n",
    "    sd_percentage=('Percentage', 'std')  \n",
    ").reset_index()\n",
    "\n",
    "custom_palette = {'NonVasc': 'gray', 'Vasc': 'lightcoral'}\n",
    "weeks = grouped['Week'].unique()\n",
    "fig, axes = plt.subplots(1, len(weeks), figsize=(20, 6), sharey=True)\n",
    "\n",
    "for i, week in enumerate(weeks):\n",
    "    ax = axes[i]\n",
    "    sns.lineplot(\n",
    "        data=grouped[grouped['Week'] == week], \n",
    "        x='Frequency (Hz)', \n",
    "        y='mean_percentage', \n",
    "        hue='Group', \n",
    "        style='Group', \n",
    "        markers=True, \n",
    "        ax=ax, \n",
    "        palette=custom_palette  # Apply custom palette\n",
    "    )\n",
    "    \n",
    "    for group in grouped['Group'].unique():\n",
    "        subset = grouped[(grouped['Week'] == week) & (grouped['Group'] == group)]\n",
    "        ax.errorbar(\n",
    "            x=subset['Frequency (Hz)'],\n",
    "            y=subset['mean_percentage'],\n",
    "            yerr=subset['sd_percentage'],\n",
    "            fmt='o',  # Marker style\n",
    "            color=custom_palette[group],  \n",
    "            capsize=5,  \n",
    "            alpha=0.5 \n",
    "        )\n",
    "    \n",
    "    ax.set_yscale('log')  \n",
    "    ax.set_ylim([1e-4, 1e2])  \n",
    "    ax.set_title(f'Week {week}')\n",
    "    ax.set_xlabel('Frequency (Hz)')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Average Percentage')\n",
    "    else:\n",
    "        ax.set_ylabel('')\n",
    "    ax.legend(title='Group')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "grouped_all_weeks = df_melted.groupby(['Group', 'Frequency (Hz)']).agg(\n",
    "    mean_percentage=('Percentage', 'mean'),\n",
    "    sd_percentage=('Percentage', 'std') \n",
    ").reset_index()\n",
    "\n",
    "plt.figure(figsize=(4, 6))\n",
    "sns.lineplot(\n",
    "    data=grouped_all_weeks, \n",
    "    x='Frequency (Hz)', \n",
    "    y='mean_percentage', \n",
    "    hue='Group', \n",
    "    style='Group', \n",
    "    markers=True, \n",
    "    ax=plt.gca(), \n",
    "    palette=custom_palette  \n",
    ")\n",
    "\n",
    "for group in grouped_all_weeks['Group'].unique():\n",
    "    subset = grouped_all_weeks[grouped_all_weeks['Group'] == group]\n",
    "    plt.errorbar(\n",
    "        x=subset['Frequency (Hz)'],\n",
    "        y=subset['mean_percentage'],\n",
    "        yerr=subset['sd_percentage'],\n",
    "        fmt='o',  # Marker style\n",
    "        color=custom_palette[group], \n",
    "        capsize=5,  \n",
    "        alpha=0.5  \n",
    "    )\n",
    "\n",
    "plt.yscale('log')  \n",
    "plt.ylim([1e-4, 1e2])  \n",
    "plt.title('Mean Percentage Across All Weeks')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Average Percentage')\n",
    "plt.legend(title='Group')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08c7bca-3dfd-4fe2-b809-1576d4157036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "file_path = r'C:\\RateHisto_Organized.xlsx'\n",
    "df = pd.read_excel(file_path, sheet_name='Frequency Percentages')\n",
    "df_melted = df.melt(id_vars=['Group', 'Week', 'Sample Name'], var_name='Frequency (Hz)', value_name='Percentage')\n",
    "df_melted['Frequency (Hz)'] = pd.to_numeric(df_melted['Frequency (Hz)'])\n",
    "df_melted = df_melted[(df_melted['Frequency (Hz)'] != 0) & (df_melted['Frequency (Hz)'] != 100)]\n",
    "grouped = df_melted.groupby(['Group', 'Week', 'Frequency (Hz)']).agg(\n",
    "    mean_percentage=('Percentage', 'mean'),\n",
    "    sd_percentage=('Percentage', 'std')  \n",
    ").reset_index()\n",
    "\n",
    "custom_palette = {'NonVasc': 'gray', 'Vasc': 'lightcoral'}\n",
    "marker_properties = {\n",
    "    'NonVasc': {'size': 3},\n",
    "    'Vasc': {'size': 3}\n",
    "}\n",
    "\n",
    "weeks = grouped['Week'].unique()\n",
    "fig, axes = plt.subplots(1, len(weeks), figsize=(10, 6), sharey=True)\n",
    "\n",
    "for i, week in enumerate(weeks):\n",
    "    ax = axes[i]\n",
    "    for group in grouped['Group'].unique():\n",
    "        subset = grouped[(grouped['Week'] == week) & (grouped['Group'] == group)]\n",
    "        properties = marker_properties[group]\n",
    "        sns.lineplot(\n",
    "            data=subset, \n",
    "            x='Frequency (Hz)', \n",
    "            y='mean_percentage', \n",
    "            hue='Group', \n",
    "            style='Group', \n",
    "            markers='o',  \n",
    "            ax=ax, \n",
    "            palette=custom_palette  \n",
    "        )\n",
    "        \n",
    "        # Plot upper error bars\n",
    "        upper_error = subset['sd_percentage']  \n",
    "        ax.errorbar(\n",
    "            x=subset['Frequency (Hz)'],\n",
    "            y=subset['mean_percentage'],\n",
    "            yerr=[np.zeros_like(upper_error), upper_error],  \n",
    "            fmt='o',  \n",
    "            color=custom_palette[group],  \n",
    "            capsize=3,  \n",
    "            alpha=0.3,  \n",
    "            #markeredgecolor=properties['edgecolor'],  \n",
    "            markersize=properties['size']  \n",
    "        )\n",
    "    \n",
    "    ax.set_yscale('log') \n",
    "    ax.set_ylim([1e-3, 1e2]) \n",
    "    ax.set_title(f'Week {week}')\n",
    "    ax.set_xlabel('Frequency (Hz)')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Average Percentage')\n",
    "    else:\n",
    "        ax.set_ylabel('')\n",
    "    ax.legend(title='Group')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "grouped_all_weeks = df_melted.groupby(['Group', 'Frequency (Hz)']).agg(\n",
    "    mean_percentage=('Percentage', 'mean'),\n",
    "    sd_percentage=('Percentage', 'std')  \n",
    ").reset_index()\n",
    "\n",
    "plt.figure(figsize=(4, 6))\n",
    "for group in grouped_all_weeks['Group'].unique():\n",
    "    subset = grouped_all_weeks[grouped_all_weeks['Group'] == group]\n",
    "    properties = marker_properties[group]\n",
    "    sns.lineplot(\n",
    "        data=subset, \n",
    "        x='Frequency (Hz)', \n",
    "        y='mean_percentage', \n",
    "        hue='Group', \n",
    "        style='Group', \n",
    "        markers='o',  \n",
    "        ax=plt.gca(), \n",
    "        palette=custom_palette  \n",
    "    )\n",
    "    \n",
    "    # Plot upper error bars\n",
    "    upper_error = subset['sd_percentage']  \n",
    "    plt.errorbar(\n",
    "        x=subset['Frequency (Hz)'],\n",
    "        y=subset['mean_percentage'],\n",
    "        yerr=[np.zeros_like(upper_error), upper_error],  \n",
    "        fmt='o',  \n",
    "        color=custom_palette[group],  \n",
    "        capsize=3,  \n",
    "        alpha=0.3,  \n",
    "        #markeredgecolor=properties['edgecolor'],  \n",
    "        markersize=properties['size']  \n",
    "    )\n",
    "\n",
    "plt.yscale('log')  \n",
    "plt.ylim([1e-3, 1e2])  \n",
    "plt.title('Mean Percentage Across All Weeks')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Average Percentage')\n",
    "plt.legend(title='Group')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data_analysis_env)",
   "language": "python",
   "name": "data_analysis_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
