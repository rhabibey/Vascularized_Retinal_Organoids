{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6920701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some features of the code may need update (allways check the spikeinterface version and updates in libraries)\n",
    "# To extract the individual neuronal unit data from raw data recorded by standard multielectrode arrays (MEAs: here multichannel systems MCS, 8x8 electrode)\n",
    "# Before using the spikeinterface the MC_Rack data should be converted to Bin format using multichanneÃ¶l systems data tool\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import spikeinterface as si  # import core only\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.preprocessing as spre\n",
    "import spikeinterface.sorters as ss\n",
    "import spikeinterface.postprocessing as spost\n",
    "import spikeinterface.qualitymetrics as sqm\n",
    "import spikeinterface.comparison as sc\n",
    "import spikeinterface.exporters as sexp\n",
    "import spikeinterface.curation as scur\n",
    "import spikeinterface.widgets as sw\n",
    "import seaborn as sns\n",
    "from spikeinterface import WaveformExtractor, extract_waveforms\n",
    "from spikeinterface import extract_waveforms\n",
    "from probeinterface import Probe, ProbeGroup\n",
    "from probeinterface.plotting import plot_probe, plot_probe_group\n",
    "from probeinterface import generate_multi_columns_probe\n",
    "from probeinterface import write_probeinterface, read_probeinterface\n",
    "from probeinterface import write_prb, read_prb\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d55b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2270d7a",
   "metadata": {},
   "source": [
    "# Sorting Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e51a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to run the first time, then can be commented out\n",
    "!git clone https://github.com/flatironinstitute/ironclust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8def84d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set path to ironclust folder\n",
    "ss.IronClustSorter.set_ironclust_path(r'D:\\......./ironclust')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e6ce9a",
   "metadata": {},
   "source": [
    "# Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca31466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In case there are sub-folders\n",
    "directory_folders = r\"D:\\........\"\n",
    "#sub_folders = [name for name in os.listdir(directory_folders) if os.path.isdir(os.path.join(directory_folders, name)) and name.endswith(\".raw\")]\n",
    "#print('directory_folders: ', directory_folders)\n",
    "#print('sub_folders: ', sub_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac94681",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = r\"D:\\.....\" \n",
    "mea_files = [file for file in os.listdir(directory_path) if file.endswith('.raw')]\n",
    "batch_size = 5\n",
    "file_counter = 0\n",
    "for i in range(0, len(mea_files), batch_size):\n",
    "    batch_files = mea_files[i:i+batch_size]\n",
    "    extractors = []\n",
    "    for mea_file in batch_files:\n",
    "        file_path = os.path.join(directory_path, mea_file)\n",
    "        recording = se.MCSRawRecordingExtractor(file_path)\n",
    "        extractors.append(recording) \n",
    "        file_counter += 1\n",
    "        print(f\"File: {mea_file}, Num channels: {recording.get_num_channels()}, Duration: {recording.get_num_frames() / recording.get_sampling_frequency()} s\")\n",
    "        print(f\"Files Read: {file_counter}\")\n",
    "    #extractors.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f3a56",
   "metadata": {},
   "source": [
    "## Setting Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc9c07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = read_probeinterface('MCS_60channel_200_30_updated_contact_ids-11-88.json')\n",
    "plot_probe_group(probe,with_contact_id=True, with_device_index=True)\n",
    "# contact ids are the names of the electrodes as idicated on the MSC map and in MC_Rack. \n",
    "# Device channel ids are the indices of the streams according to the wiring.\n",
    "probe_df=probe.to_dataframe(complete=True).loc[:, [\"contact_ids\", \"device_channel_indices\"]]\n",
    "display(probe_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f8c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude channels that showe high noise levels\n",
    "\"\"\"\n",
    "exc_channels = [\n",
    "    [42,84],\n",
    "    [46]\n",
    "]\n",
    "\"\"\"\n",
    "exc_channels = [\n",
    "    [15], [21],[31],[41],[51],[61],[71],[12],[22],[32],[42],[52],[62],[]\n",
    "]\n",
    "\n",
    "exc_channels_ind = []\n",
    "for i in range(len(exc_channels)):\n",
    "    indices = []\n",
    "    for k in range(len(exc_channels[i])):\n",
    "        indices.append(probe_df[\"device_channel_indices\"][int(np.where(probe_df[\"contact_ids\"] == str(exc_channels[i][k]))[0])])\n",
    "    exc_channels_ind.append(indices)\n",
    "    \n",
    "print(exc_channels_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14442c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include selected channels in sorting\n",
    "\n",
    "inc_channels = [16, 17, 25, 26, 27, 28,\n",
    "                35, 36, 37, 38, 45, 46, 47, 48, \n",
    "                55, 56, 57, 58, 65, 66, 67, 68, \n",
    "                75, 76, 77, 78, 85, 86, 87] \n",
    "for i in inc_channels:\n",
    "    condition = probe_df[\"contact_ids\"] == str(i)\n",
    "    indices = np.where(condition)[0] \n",
    "    if len(indices) > 0:\n",
    "        index = indices[0]\n",
    "        inc_channels_ind.append(probe_df[\"device_channel_indices\"].iloc[index])\n",
    "    else:\n",
    "        print(f\"No match found for {i}\")\n",
    "print(inc_channels_ind)\n",
    "inc_channels_ind.sort()\n",
    "print(inc_channels_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad456e1",
   "metadata": {},
   "source": [
    "# Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20535f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options for parallel processing:\n",
    "n_workers = 20\n",
    "chunk_memory = \"1000M\"\n",
    "\n",
    "# options for automatic curation/quality control\n",
    "snr_thresh = 4.5\n",
    "isi_viol_thresh = 0.2\n",
    "#query = f\"snr > {snr_thresh} & isi_violations_rate < {isi_viol_thresh}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80722880",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_job_kwargs = dict(n_jobs=20, chunk_duration=\"1s\")\n",
    "si.set_global_job_kwargs(**global_job_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4965bbf-ee7d-4bf0-8811-884805261133",
   "metadata": {},
   "source": [
    "# Sorting in Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f7e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Sorting in Steps ##### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3becc05-d87b-4f05-b72c-524d4562258d",
   "metadata": {},
   "source": [
    "### Step 01 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c05e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 01 Preprocessing\n",
    "\n",
    "# List to keep track of files that had errors\n",
    "error_files_preprocessing = []\n",
    "for mea_file in mea_files:\n",
    "    try:\n",
    "        print(f\"Processing file: {mea_file}\")\n",
    "        \n",
    "        file_path = os.path.join(directory_path, mea_file)\n",
    "        recording = se.MCSRawRecordingExtractor(file_path)\n",
    "        print(recording)\n",
    "        recording.annotate(is_filtered=False)\n",
    "        channel_ids = recording.get_channel_ids()\n",
    "        fs = recording.get_sampling_frequency()\n",
    "        num_chan = recording.get_num_channels()\n",
    "        num_segments = recording.get_num_segments()\n",
    "        #print(f'Channel ids: {channel_ids}')\n",
    "        #print(f'Sampling frequency: {fs}')\n",
    "        #print(f'Number of channels: {num_chan}')\n",
    "        #print(f\"Number of segments: {num_segments}\")\n",
    "        \n",
    "        # Set probes and preprocess\n",
    "        recording_prb = recording.set_probes(probe)\n",
    "        recording_f = spre.filter(recording_prb, band=100, btype=\"highpass\", filter_mode=\"sos\", ftype='butter')\n",
    "        recording_cmr = spre.common_reference(recording_f, reference='global', operator='median')\n",
    "        \n",
    "        # Save preprocessed recording\n",
    "        rec_preprocessed = recording_cmr.save(folder=f\"preprocessed_{mea_file}\", progress_bar=False, n_jobs=n_workers, chunk_memory=chunk_memory)\n",
    "        print(f\"Preprocessing and saving completed for file: {mea_file}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {mea_file}: {e}\")\n",
    "        error_files_preprocessing.append(mea_file)\n",
    "\n",
    "# list the files that had errors\n",
    "if error_files_preprocessing:\n",
    "    print(\"\\nThe following files had errors and were not processed successfully:\")\n",
    "    for error_file in error_files_preprocessing:\n",
    "        print(error_file)\n",
    "else:\n",
    "    print(\"\\nAll files were processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7e431a-8e1e-4cfa-b2d0-9c525ef5e7b5",
   "metadata": {},
   "source": [
    "### Step 02 Results and Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c18c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 02 Results and Sorting\n",
    "# Sort preprocessed data using IronClust and save sorted data\n",
    "\n",
    "error_files = []\n",
    "for mea_file in mea_files:\n",
    "    try:\n",
    "        print(f\"Processing file: {mea_file}\")\n",
    "        # Load the preprocessed folder\n",
    "        recording_loaded = si.load_extractor(f\"preprocessed_{mea_file}/\")\n",
    "        #print(f\"Loaded recording: {recording_loaded}\")\n",
    "        # Define output folder for IronClust results\n",
    "        output_folder = f\"results_IC_{mea_file}\"\n",
    "        #print(f\"Output folder: {output_folder}\")\n",
    "\n",
    "        # Run IronClust sorter\n",
    "        recording_IC = ss.run_sorter('ironclust', \n",
    "                                     recording_loaded, \n",
    "                                     output_folder=output_folder, \n",
    "                                     detect_threshold=5, \n",
    "                                     verbose=True)\n",
    "        \n",
    "        # Save the sorted data\n",
    "        recording_IC_saved = recording_IC.save(folder=f\"sorting_IC_{mea_file}\")\n",
    "        print(f\"Saved sorted data to: sorting_IC_{mea_file}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {mea_file}: {e}\")\n",
    "        error_files.append(mea_file)\n",
    "\n",
    "# list the files that had errors\n",
    "if error_files:\n",
    "    print(\"\\nThe following files had errors and were not processed successfully:\")\n",
    "    for error_file in error_files:\n",
    "        print(error_file)\n",
    "else:\n",
    "    print(\"\\nAll files were processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3699db24-dac1-4c52-8db7-7cea5140df0d",
   "metadata": {},
   "source": [
    "### Step 03 Autocuration and exporting units timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3984a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 03 Autocuration and export the .CSV of sorted timestamps\n",
    "# Automatic curation according to SNR threshold and ISI violation \n",
    "\n",
    "# List the files that had errors\n",
    "error_files_autocuration = []\n",
    "error_files_csv = []\n",
    "\n",
    "for mea_file in mea_files:\n",
    "    try:\n",
    "        print(f\"Processing autocuration for file: {mea_file}\")\n",
    "        recording_loaded = si.load_extractor(f\"preprocessed_{mea_file}/\")\n",
    "        sorting_loaded = si.load_extractor(f\"sorting_IC_{mea_file}/\")\n",
    "        recording_we = si.extract_waveforms(recording_loaded, sorting_loaded, folder=f\"wf_{mea_file}\", progress_bar=False, n_jobs=n_workers, chunk_memory=chunk_memory, overwrite=True)\n",
    "        recording_qc = si.qualitymetrics.compute_quality_metrics(recording_we)\n",
    "        keep_units_recording = recording_qc.query(\"snr > 4.5\")\n",
    "        keep_unit_ids_recording = keep_units_recording.index.values\n",
    "        recording_sorting_autocur = sorting_loaded.select_units(keep_unit_ids_recording)\n",
    "        recording_sorting_autocur.save(folder=f\"autocurated_sorting_IC_{mea_file}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during autocuration for {mea_file}: {e}\")\n",
    "        error_files_autocuration.append(mea_file)\n",
    "\n",
    "# Export timestamp information for each recording into a .csv file\n",
    "for mea_file in mea_files:\n",
    "    try:\n",
    "        print(f\"Exporting timestamps to CSV for file: {mea_file}\")\n",
    "        sorting = si.load_extractor(f\"autocurated_sorting_IC_{mea_file}/\")\n",
    "        n = len(sorting.get_unit_ids())\n",
    "        data = {}\n",
    "        for k in range(n):\n",
    "            unit_id = sorting.get_unit_ids()[k]\n",
    "            data[str(unit_id)] = sorting.get_unit_spike_train(unit_id)\n",
    "        df = pd.DataFrame.from_dict(data, orient='index').transpose()\n",
    "        df.to_csv(f'autocurated_timestamps_{mea_file}.csv')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while exporting timestamps for {mea_file}: {e}\")\n",
    "        error_files_csv.append(mea_file)\n",
    "\n",
    "# list the files that had errors\n",
    "if error_files_autocuration:\n",
    "    print(\"\\nThe following files had errors during autocuration and were not processed successfully:\")\n",
    "    for error_file in error_files_autocuration:\n",
    "        print(error_file)\n",
    "else:\n",
    "    print(\"\\nAll files were autocurated successfully.\")\n",
    "\n",
    "if error_files_csv:\n",
    "    print(\"\\nThe following files had errors during CSV export and were not processed successfully:\")\n",
    "    for error_file in error_files_csv:\n",
    "        print(error_file)\n",
    "else:\n",
    "    print(\"\\nAll files were exported to CSV successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979206ab-9aeb-4e5a-aca6-61269406a3d5",
   "metadata": {},
   "source": [
    "### Step 04 Extraction of Unit IDs and Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a49ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 04 Unit ID and Position\n",
    "# extract unit ids and positions into .csv file\n",
    "\n",
    "# List the files that had errors\n",
    "error_files_unit_position = []\n",
    "\n",
    "for mea_file in mea_files:\n",
    "    try:\n",
    "        print(f\"Processing unit IDs and positions for file: {mea_file}\")\n",
    "        # Load preprocessed recording and autocurated sorting\n",
    "        recording_loaded = si.load_extractor(f\"preprocessed_{mea_file}/\")\n",
    "        sorting_autocur_loaded = si.load_extractor(f\"autocurated_sorting_IC_{mea_file}/\")\n",
    "        \n",
    "        recording_we = si.extract_waveforms(recording_loaded, sorting_autocur_loaded, \n",
    "                                            folder=f\"wf_{mea_file}\", progress_bar=True, n_jobs=n_workers, \n",
    "                                            chunk_memory=chunk_memory, overwrite=True)\n",
    "        # Extract unit IDs and positions\n",
    "        unit_ids = sorting_autocur_loaded.get_unit_ids()\n",
    "        print(f\"{mea_file}: Unit IDs - {unit_ids}, number of units: {len(unit_ids)}\")\n",
    "        unit_loc = spost.compute_unit_locations(recording_we)\n",
    "        print(f\"{mea_file}: Unit locations - {unit_loc}, number of locations: {len(unit_loc)}\")\n",
    "        \n",
    "        # Prepare data for DataFrame\n",
    "        data = {'unit_id': [], 'x': [], 'y': [], 'z': []}\n",
    "        for unit_id, position in zip(unit_ids, unit_loc):\n",
    "            data['unit_id'].append(unit_id)\n",
    "            data['x'].append(position[0])  \n",
    "            data['y'].append(position[1])  \n",
    "            data['z'].append(position[2])  \n",
    "        df = pd.DataFrame(data)\n",
    "        print(f\"{mea_file}: DataFrame -\\n{df}, number of units: {len(df)}\")\n",
    "        df.to_csv(os.path.join(directory_path, f\"unit_positions_{mea_file}.csv\"), index=False)\n",
    "        print(f\"{mea_file}: CSV export completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {mea_file}: {e}\")\n",
    "        error_files_unit_position.append(mea_file)\n",
    "\n",
    "#list the files that had errors\n",
    "if error_files_unit_position:\n",
    "    print(\"\\nThe following files had errors and were not processed successfully:\")\n",
    "    for error_file in error_files_unit_position:\n",
    "        print(error_file)\n",
    "else:\n",
    "    print(\"\\nAll files were processed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
