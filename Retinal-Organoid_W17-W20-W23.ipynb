{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retinal Organoid and Stim Data\n",
    "#### Reads digital file of stim, reads neuron timestamps, merges data, imprt them to neuroexplorer, gets neuroexplorer output, categorizes groups and plots the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 01 _ Read txt file of stim Digit file and extract On and OFF times in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Whole code to: \n",
    "- Read txt files of digital bit files,  \n",
    "- Convert to excel \n",
    "- Splits Stim time points to applied Freq and pulse width (each sheet contains specific Freq and pulse width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Step 01 _ Read txt file of stim Digit file and extract the pulse-start and pulse-end times\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def extract_stimulus_events(txt_file):\n",
    "    stim_on = []  # List to store stimulus onset times\n",
    "    stim_off = []  # List to store stimulus offset times\n",
    "    \n",
    "    with open(txt_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        lines = lines[2:]\n",
    "        \n",
    "        for i in range(1, len(lines)):  # Start from the second row\n",
    "            try:\n",
    "                prev_value = int(lines[i-1].split()[1])\n",
    "                curr_value = int(lines[i].split()[1])\n",
    "                if prev_value == 0 and curr_value == 4:\n",
    "                    stim_on.append(float(lines[i].split()[0]) / 1000)  # Convert to seconds\n",
    "                elif prev_value == 4 and curr_value == 0:\n",
    "                    stim_off.append(float(lines[i].split()[0]) / 1000)  # Convert to seconds\n",
    "            except (ValueError, IndexError):\n",
    "                continue  \n",
    "    \n",
    "    return stim_on, stim_off\n",
    "\n",
    "def export_to_excel(txt_file, stim_on, stim_off):\n",
    "    file_name = os.path.splitext(txt_file)[0]\n",
    "    df = pd.DataFrame({'Stim_ON': stim_on, 'Stim_OFF': stim_off})\n",
    "    output_file = f\"{file_name}_stimulus_times.xlsx\"\n",
    "    df.to_excel(output_file, index=False)\n",
    "    return output_file\n",
    "\n",
    "def split_data_and_save_to_excel(input_file, output_file):\n",
    "    df = pd.read_excel(input_file)\n",
    "    categories = ['50ms_0.2Hz', '50ms_1.0Hz', '50ms_2.0Hz', '50ms_5.0Hz']\n",
    "    start_indices = [2, 7, 17, 37] # This depends on light stim protocol (e.g. pulse 2 to 6 is 50ms_0.5Hz)\n",
    "    end_indices = [6, 16, 36, 61]  \n",
    "    \n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        for cat, start, end in zip(categories, start_indices, end_indices):\n",
    "            cat_df = df.iloc[start-2:end-1]  \n",
    "            cat_df.to_excel(writer, sheet_name=cat, index=False)\n",
    "\n",
    "def process_files_and_split_data(txt_folder):\n",
    "    for txt_file in os.listdir(txt_folder):\n",
    "        if txt_file.endswith(\".txt\"):\n",
    "            txt_path = os.path.join(txt_folder, txt_file)\n",
    "            stim_on, stim_off = extract_stimulus_events(txt_path)\n",
    "            excel_file = export_to_excel(txt_path, stim_on, stim_off)\n",
    "            split_data_and_save_to_excel(excel_file, f\"{os.path.splitext(txt_path)[0]}_splitted.xlsx\")\n",
    "            print(f\"Processed file: {txt_file}\")\n",
    "\n",
    "# Example usage:\n",
    "txt_folder = r'C:\\StimTimes_Digital_Txt' \n",
    "process_files_and_split_data(txt_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 02 _ Merge Stim ON-OFF file with .CSV of Unit time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Merges the .CSV file of unit time series with .xlsx file of StimTimestamps_Splitted (from previous step).\n",
    "- Cosider that stimtimestamps are in seconds but neuron timestamps still not. this will be fixed in next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge Pulse timestamps with neuronal timestamp file\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def extract_data_from_excel(excel_file):\n",
    "    sheets_data = pd.read_excel(excel_file, sheet_name=None)\n",
    "    \n",
    "    extracted_data = {}\n",
    "    for sheet_name, sheet_df in sheets_data.items():\n",
    "        extracted_data[sheet_name] = sheet_df.iloc[:, :2]\n",
    "    return extracted_data\n",
    "\n",
    "def add_sheet_names_to_columns(data):\n",
    "    for sheet_name, sheet_data in data.items():\n",
    "        new_column_names = [f\"{sheet_name}_{col}\" for col in sheet_data.columns]\n",
    "        sheet_data.columns = new_column_names\n",
    "    return data\n",
    "\n",
    "def merge_excel_and_csv(excel_data, csv_file):\n",
    "    csv_data = pd.read_csv(csv_file)\n",
    "    merged_data = pd.concat([pd.concat(excel_data.values(), axis=1), csv_data], axis=1)\n",
    "    return merged_data\n",
    "\n",
    "def save_to_csv(output_csv_file, merged_data):\n",
    "    merged_data.to_csv(output_csv_file, index=False)\n",
    "\n",
    "# Load pulse timestamp file and unit timestamp file\n",
    "excel_folder = r\"C:\\StimTimestamps\" \n",
    "csv_folder = r\"C:\\UnitTimeseries\"  \n",
    "output_folder = r\"C:\\Stim-Unit_Timestamps\"  \n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for excel_file in os.listdir(excel_folder):\n",
    "    if excel_file.endswith(\".xlsx\"):\n",
    "        excel_path = os.path.join(excel_folder, excel_file)\n",
    "        extracted_data = extract_data_from_excel(excel_path)\n",
    "        add_sheet_names_to_columns(extracted_data)\n",
    "        \n",
    "        for csv_file in os.listdir(csv_folder):\n",
    "            if csv_file.endswith(\".csv\") and excel_file[:28] in csv_file:\n",
    "                csv_path = os.path.join(csv_folder, csv_file)\n",
    "                merged_data = merge_excel_and_csv(extracted_data, csv_path)\n",
    "                output_csv_file_name = os.path.splitext(csv_file)[0] + \"_merged.csv\"\n",
    "                output_csv_file_path = os.path.join(output_folder, output_csv_file_name)\n",
    "                save_to_csv(output_csv_file_path, merged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 03 _ Make data ready for NEX by multiplying stim times in sampling frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this code It reads all Stim_timestamps from Merged file and multiply them with Sampling Frequency. \n",
    "- It also removes the index column from column 15\n",
    "- The rest of dataset is returned as it is\n",
    "- It is required for loading the data into Neuroexplorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare data to for importing to Neuroexplorer (Nex). Later these data will be used to extract PSTH data\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def adjust_sampling_frequency(csv_file, sampling_frequency):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df.iloc[0:, 0:14] *= sampling_frequency #Stimtimes*samplingFreq\n",
    "    df.drop(columns=df.columns[14], inplace=True)\n",
    "    return df\n",
    "\n",
    "def process_files_and_save_to_csv(csv_folder, output_folder, sampling_frequency):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        \n",
    "    for csv_file in os.listdir(csv_folder):\n",
    "        if csv_file.endswith(\".csv\"):\n",
    "            csv_path = os.path.join(csv_folder, csv_file)\n",
    "            df_adjusted = adjust_sampling_frequency(csv_path, sampling_frequency)\n",
    "            output_file_name = os.path.splitext(csv_file)[0] + \"_stimXsampFreq.csv\"\n",
    "            output_file = os.path.join(output_folder, output_file_name)\n",
    "            df_adjusted.to_csv(output_file, index=False)\n",
    "\n",
    "csv_folder = r\"C:\\Merged\"  \n",
    "output_folder = r\"C:\\Merged_StimXsampFreq\"  \n",
    "sampling_frequency = 32000  # Specify the sampling frequency\n",
    "\n",
    "process_files_and_save_to_csv(csv_folder, output_folder, sampling_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "Merged_StimXsampFreq is imported to NEX to extract the Burst, PSTH, RateHisto data, .....\n",
    "Automated NEX scripts runs a template algorithems (adjusted by user) to extract PSTH, Burst, .... data\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 04 Example NexCode for extracting PSTH, Burst, RateHisto from many files (each file is data of one sample in one specific day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% This is a neuroexplorer script ( .nsc ) \n",
    "% which reads many excel files containing neurons and their timestamps and stimulation time stamps as well and extracts PSTH data\n",
    "% read All files in one folder, run analysis, export data for each MEA in a separate file and for each week in a separate sheet\n",
    "% After generation of templates for each variable , copy and paste the code in the neuroexplorer script, define directory, run the code in NEX\n",
    "\n",
    "SourcePath=\"C:\\folder\\\"     % Enter the directory with the source data mcd or nex. Do NOT FORGET a backslash at the end!\n",
    "SourceFilter = SourcePath + \"*.csv\"             % Process files with given extension (e.g., .nex or .mcd) only\n",
    "\n",
    "FileCount = GetFileCount(SourceFilter)\n",
    "Trace(FileCount, \" files to analyze\")           \n",
    "\n",
    "for i= 1 to FileCount\n",
    "\tname = GetFileName(i)\n",
    "\tTrace(name)                                 % prints \"the name\" of the all files ready to be analysed\n",
    "\tdoc = OpenDocument(name)\n",
    "\tTitle = GetDocTitle(doc)\n",
    "\t\n",
    "    if doc > 0\n",
    "        SelectAllNeurons(doc)                                                                \n",
    "        %ApplyTemplate(doc, \"Burst__definedbyuser_01\")        % Burst detection template has already been generated and saved in template folder of NEX\n",
    "        ApplyTemplate(doc, \"PSTH_Kritika__definedbyuser_01\")  % PSTH template has already been generated and saved in template folder of NEX  \n",
    "        %ApplyTemplate(doc, \"RateHistogram_definedbyuser_01\") % Rate Histo template has already been generated and saved in template folder of NEX              \n",
    "        \n",
    "        ExcelFileName= \"PSTH\"+ Left(Title,1)    % the name of the output file in each loop \n",
    "        Trace(ExcelFileName)                    % prints the MEA for which the data is extracting\n",
    "        ExcelSheetName= Left (Title,25)         % the name of the output sheet in each loop\n",
    "        Trace(ExcelSheetName)                   % prints the MEA for which the data is extracting\n",
    "        \n",
    "        % save all data\n",
    "        %SendResultsToExcel(doc,\"C:\\PSTH_data_01.xlsx\", ExcelSheetName, 0, \"A1\", 1, 1)\n",
    "        % save only Summary \n",
    "        SendResultsSummaryToExcel(doc,\"C:\\PSTH_data_02.xlsx\", ExcelSheetName, 0, \"A1\", 1, 1) \n",
    "        CloseDocument(doc)                      % prevents from accumulation of open files in Nex\n",
    "     end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 05 xlsx output of Nex data is loaded in following codes for further categorization and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the details of light stimulation protocol in the 'Variable' rows and extract average data of each protocol for all neurons\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "## Read the Excel file (ouput of Nex for PSTH)\n",
    "input_excel_file = r\"C:\\PSTH_data.xlsx\"\n",
    "xls = pd.ExcelFile(input_excel_file)\n",
    "input_directory = os.path.dirname(input_excel_file)\n",
    "output_excel_file = os.path.join(input_directory, 'PSTH_Processed.xlsx')\n",
    "\n",
    "first_sheet_name = xls.sheet_names[0]\n",
    "df_first_sheet = pd.read_excel(xls, first_sheet_name, header=0) \n",
    "variable_names = df_first_sheet.columns.tolist()\n",
    "print(f'variable_names', variable_names)\n",
    "\n",
    "processed_sheets = {}\n",
    "for sheet_name in xls.sheet_names:\n",
    "    df = pd.read_excel(xls, sheet_name)\n",
    "    if df.empty:\n",
    "        print(f\"Sheet '{sheet_name}' is empty. Skipping.\")\n",
    "        continue\n",
    "    filtered_data = df[~df['Variable'].str.startswith(('50ms'))].copy()\n",
    "    filtered_data['Neuron'] = filtered_data['Variable'].apply(lambda x: x.split(' vs. ')[0].split()[0])\n",
    "    filtered_data['Protocol'] = filtered_data['Variable'].apply(lambda x: x.split(' vs. ')[1])\n",
    "    filtered_data['Pulse_Width'] = filtered_data['Protocol'].apply(lambda x: x.split('_')[0])\n",
    "    filtered_data['Frequency'] = filtered_data['Protocol'].apply(lambda x: x.split('_')[1].split('Hz')[0])\n",
    "    \n",
    "    # Group by protocol and calculate averages for desired variables\n",
    "    grouped_data = filtered_data.groupby(['Protocol', 'Pulse_Width', 'Frequency']).agg({\n",
    "        'NumRefEvents': 'mean',\n",
    "        'Spikes': 'mean',\n",
    "        'Filter Length': 'mean',\n",
    "        'Mean Freq.': 'mean',\n",
    "        'Mean Hist.': 'mean',\n",
    "        'Z-score Mean': 'mean',\n",
    "        'Peak Z-score': 'mean',\n",
    "        'Peak/Mean': 'mean',\n",
    "        'Peak Position': 'mean',\n",
    "        'Peak Half Height': 'mean',\n",
    "        'Peak Width at Half Height': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    processed_sheets[sheet_name] = grouped_data\n",
    "\n",
    "\n",
    "with pd.ExcelWriter(output_excel_file) as writer:\n",
    "    for sheet_name, processed_data in processed_sheets.items():\n",
    "        processed_data.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"Processed data saved to {output_excel_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 06 further categorization steps, preparing data for Graphpad prism, and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adds a column to the each dataset column name=seet name (sample name)\n",
    "import pandas as pd\n",
    "\n",
    "def modify_sheet_names_and_add_sample_name(excel_file):\n",
    "    # Read the Excel file with all sheets\n",
    "    xls = pd.ExcelFile(excel_file)\n",
    "    sheet_names = xls.sheet_names\n",
    "    modified_sheet_names = [name.replace(\".raw_m\", \"\") for name in sheet_names]\n",
    "    return xls, modified_sheet_names\n",
    "def copy_data_to_new_excel_with_sample_name(xls, modified_sheet_names, output_excel):\n",
    "    with pd.ExcelWriter(output_excel) as writer:\n",
    "        for sheet_name, mod_sheet_name in zip(xls.sheet_names, modified_sheet_names):\n",
    "            df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "            df.insert(0, \"Sample Name\", sheet_name)\n",
    "            df.to_excel(writer, sheet_name=mod_sheet_name, index=False)\n",
    "\n",
    "excel_file = r\"C:\\PSTH_Processed.xlsx\" \n",
    "output_excel = r\"C:\\PSTH_Processed_02.xlsx\"  \n",
    "\n",
    "xls, modified_names = modify_sheet_names_and_add_sample_name(excel_file)\n",
    "copy_data_to_new_excel_with_sample_name(xls, modified_names, output_excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatinate all data\n",
    "import pandas as pd\n",
    "file_path = r\"C:\\PSTHMerged_Processed_02.xlsx\"\n",
    "xls = pd.ExcelFile(file_path)\n",
    "suffixes = ['_02', '_03', '_04']\n",
    "concatenated_sheets = {}\n",
    "for suffix in suffixes:\n",
    "    relevant_sheets = [sheet_name for sheet_name in xls.sheet_names if sheet_name.endswith(suffix)]\n",
    "    concatenated_df = pd.concat([pd.read_excel(xls, sheet_name) for sheet_name in relevant_sheets])\n",
    "    concatenated_sheets[f'Intensity{suffix}'] = concatenated_df\n",
    "\n",
    "output_file_path = r\"C:\\ConcatPSTH.xlsx\"\n",
    "with pd.ExcelWriter(output_file_path) as writer:\n",
    "    for sheet_name, df in concatenated_sheets.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupp samples based on Week\n",
    "import pandas as pd\n",
    "\n",
    "file_path = r\"C:\\ConcatPSTH.xlsx\"\n",
    "xls = pd.ExcelFile(file_path)\n",
    "processed_sheets = {}\n",
    "for sheet_name in xls.sheet_names:\n",
    "    df = pd.read_excel(xls, sheet_name)\n",
    "    df['Week'] = df['Sample Name'].str[:3]\n",
    "    df['Group'] = df['Sample Name'].apply(lambda x: 'Vascularized' if 'VASCU' in x else 'Non_Vascularized')\n",
    "    new_columns = ['Group', 'Week'] + [col for col in df.columns if col not in ['Group', 'Week']]\n",
    "    df = df[new_columns]\n",
    "    processed_sheets[sheet_name] = df\n",
    "output_file_path = r\"C:\\Groupped_ConcatPSTH.xlsx\"\n",
    "with pd.ExcelWriter(output_file_path) as writer:\n",
    "    for sheet_name, df in processed_sheets.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot data\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "colors = {'Non_Vascularized': 'gray', 'Vascularized': 'lightcoral'}\n",
    "\n",
    "file_path = r\"C:\\Groupped_ConcatPSTH.xlsx\"\n",
    "xls = pd.ExcelFile(file_path)\n",
    "\n",
    "## remove outliers\n",
    "def remove_outliers(df, column):\n",
    "    def _remove_outliers(group):\n",
    "        Q1 = group[column].quantile(0.25)\n",
    "        Q3 = group[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        return group[~((group[column] < (Q1 - 3 * IQR)) | (group[column] > (Q3 + 5 * IQR)))]\n",
    "    return df.groupby(['Group', 'Week']).apply(_remove_outliers).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def calculate_group_statistics(df):\n",
    "    group_stats = df.groupby(['Week', 'Group'])['Mean Hist.'].agg(['mean', 'std', 'sem', 'max', 'min', 'count']).reset_index()\n",
    "    group_counts = df.groupby('Group')['Mean Hist.'].count().reset_index()\n",
    "    group_stats['Count'] = group_counts['Mean Hist.']\n",
    "    return group_stats\n",
    "\n",
    "processed_data = {}\n",
    "variable_stats_raw = {}\n",
    "variable_stats_cleaned = {}\n",
    "for sheet_name in xls.sheet_names:\n",
    "    df = pd.read_excel(xls, sheet_name)\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['Week', 'Mean Hist.'])\n",
    "    group_stats_raw = calculate_group_statistics(df)\n",
    "    variable_stats_raw[sheet_name] = group_stats_raw\n",
    "    \n",
    "    # --------------------- Remove outliers--------------------\n",
    "    df_cleaned = remove_outliers(df, 'Mean Hist.')\n",
    "    processed_data[sheet_name] = df_cleaned\n",
    "    group_stats = calculate_group_statistics(df_cleaned)\n",
    "    variable_stats_cleaned[sheet_name] = group_stats\n",
    "    group_order = ['Non_Vascularized', 'Vascularized']\n",
    "    plt.figure(figsize=(7, 6))  \n",
    "    ax=sns.boxplot(data=df_cleaned, x='Week', y='Mean Hist.', hue='Group', \n",
    "                order=['w17', 'w20', 'w23'],\n",
    "                hue_order=group_order, palette=colors, showfliers=False)  \n",
    "    sns.stripplot(data=df_cleaned, x='Week', y='Mean Hist.', hue='Group', \n",
    "                  order=['w17', 'w20', 'w23'],\n",
    "                  hue_order=group_order, dodge=True, jitter=True, edgecolor='black', linewidth=0.5, palette=colors, \n",
    "                  alpha=0.7, size=4) \n",
    "    means = df_cleaned.groupby(['Week', 'Group'])['Mean Hist.'].mean().reset_index()\n",
    "    for group, color in colors.items():\n",
    "        plt.plot(means[means['Group'] == group]['Week'], means[means['Group'] == group]['Mean Hist.'], \n",
    "                 color=color, linestyle='-', linewidth=2, marker='o', markersize=5, markeredgecolor='black',\n",
    "                 markeredgewidth=3)      \n",
    "    for group, color in colors.items():\n",
    "        sems = df_cleaned.groupby(['Week', 'Group'])['Mean Hist.'].sem().reset_index()\n",
    "        group_sems = sems[sems['Group'] == group]\n",
    "        for dpi in ['w17', 'w20', 'w23']:\n",
    "            dpi_sems = group_sems[group_sems['Week'] == dpi]\n",
    "            plt.errorbar(dpi_sems['Week'], means[(means['Group'] == group) & (means['Week'] == dpi)]['Mean Hist.'], \n",
    "                         yerr=dpi_sems['Mean Hist.'], fmt='o', color=color, markersize=10, capsize=5, \n",
    "                         markeredgecolor='black', linewidth=3)  \n",
    "    \n",
    "    plt.xticks(rotation=45, fontsize=18)\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylim(1e-1, 1e3) \n",
    "    plt.legend(title='Group', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xlabel('Week', fontsize=16)  \n",
    "    plt.ylabel('Mean Hist.', fontsize=18)  \n",
    "    plt.title(f'{sheet_name}', fontsize=18)  \n",
    "    plt.yticks(fontsize=16)  \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "output_excel_path = r'C:\\Groupped_ConcatPSTH_Stat.xlsx'\n",
    "with pd.ExcelWriter(output_excel_path) as writer:\n",
    "    for variable, stats in variable_stats_raw.items():\n",
    "        stats.to_excel(writer, sheet_name=f\"{variable}_raw\", index=False)\n",
    "    for variable, stats in variable_stats_cleaned.items():\n",
    "        stats.to_excel(writer, sheet_name=f\"{variable}_clean\", index=False)\n",
    "output_excel_path_cleaned = r'C:\\Groupped_ConcatPSTH_OutliersRemoved.xlsx'\n",
    "with pd.ExcelWriter(output_excel_path_cleaned) as writer_cleaned:\n",
    "    for sheet_name, data in processed_data.items():\n",
    "        data.to_excel(writer_cleaned, sheet_name=sheet_name, index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
