{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retinal Organoid Stim (ResponsiveNeurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Digital data of light pulses (extracted by spikeinterface) is extracted and processed\n",
    "- This Jupyter Notebook is complete note book that extracts and categorize data of baseline and stimulation from .CSV extracts of spike interface.\n",
    "- The first steps of this notebook is dealing with merging the stim timestamps with unit timeseries \n",
    "- Then the code will make data ready for NEX burst, PSTH and FiringHisto analysis\n",
    "- The results of NEX is later imported to categorize, plot and make it ready for Prism statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Step 01 _ Read txt file of stim Digit file and extract the pulse-start and pulse-end times\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def extract_stimulus_events(txt_file):\n",
    "    stim_on = []  # List to store stimulus onset times\n",
    "    stim_off = []  # List to store stimulus offset times\n",
    "    \n",
    "    with open(txt_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        lines = lines[2:]\n",
    "        \n",
    "        for i in range(1, len(lines)):  # Start from the second row\n",
    "            try:\n",
    "                prev_value = int(lines[i-1].split()[1])\n",
    "                curr_value = int(lines[i].split()[1])\n",
    "                if prev_value == 0 and curr_value == 4:\n",
    "                    stim_on.append(float(lines[i].split()[0]) / 1000)  # Convert to seconds\n",
    "                elif prev_value == 4 and curr_value == 0:\n",
    "                    stim_off.append(float(lines[i].split()[0]) / 1000)  # Convert to seconds\n",
    "            except (ValueError, IndexError):\n",
    "                continue  \n",
    "    \n",
    "    return stim_on, stim_off\n",
    "\n",
    "def export_to_excel(txt_file, stim_on, stim_off):\n",
    "    file_name = os.path.splitext(txt_file)[0]\n",
    "    df = pd.DataFrame({'Stim_ON': stim_on, 'Stim_OFF': stim_off})\n",
    "    output_file = f\"{file_name}_stimulus_times.xlsx\"\n",
    "    df.to_excel(output_file, index=False)\n",
    "    return output_file\n",
    "\n",
    "def split_data_and_save_to_excel(input_file, output_file):\n",
    "    df = pd.read_excel(input_file)\n",
    "    categories = ['50ms_0.2Hz', '50ms_1.0Hz', '50ms_2.0Hz', '50ms_5.0Hz']\n",
    "    start_indices = [2, 7, 17, 37] # This depends on light stim protocol (e.g. pulse 2 to 6 is 50ms_0.5Hz)\n",
    "    end_indices = [6, 16, 36, 61]  \n",
    "    \n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        for cat, start, end in zip(categories, start_indices, end_indices):\n",
    "            cat_df = df.iloc[start-2:end-1]  \n",
    "            cat_df.to_excel(writer, sheet_name=cat, index=False)\n",
    "\n",
    "def process_files_and_split_data(txt_folder):\n",
    "    for txt_file in os.listdir(txt_folder):\n",
    "        if txt_file.endswith(\".txt\"):\n",
    "            txt_path = os.path.join(txt_folder, txt_file)\n",
    "            stim_on, stim_off = extract_stimulus_events(txt_path)\n",
    "            excel_file = export_to_excel(txt_path, stim_on, stim_off)\n",
    "            split_data_and_save_to_excel(excel_file, f\"{os.path.splitext(txt_path)[0]}_splitted.xlsx\")\n",
    "            print(f\"Processed file: {txt_file}\")\n",
    "\n",
    "# Example usage:\n",
    "txt_folder = r'C:\\StimTimes_Digital_Txt' \n",
    "process_files_and_split_data(txt_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge Pulse timestamps with neuronal timestamp file\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def extract_data_from_excel(excel_file):\n",
    "    sheets_data = pd.read_excel(excel_file, sheet_name=None)\n",
    "    \n",
    "    extracted_data = {}\n",
    "    for sheet_name, sheet_df in sheets_data.items():\n",
    "        extracted_data[sheet_name] = sheet_df.iloc[:, :2]\n",
    "    return extracted_data\n",
    "def add_sheet_names_to_columns(data):\n",
    "    for sheet_name, sheet_data in data.items():\n",
    "        new_column_names = [f\"{sheet_name}_{col}\" for col in sheet_data.columns]\n",
    "        sheet_data.columns = new_column_names\n",
    "    return data\n",
    "def merge_excel_and_csv(excel_data, csv_file):\n",
    "    csv_data = pd.read_csv(csv_file)\n",
    "    merged_data = pd.concat([pd.concat(excel_data.values(), axis=1), csv_data], axis=1)\n",
    "    return merged_data\n",
    "def save_to_csv(output_csv_file, merged_data):\n",
    "    merged_data.to_csv(output_csv_file, index=False)\n",
    "\n",
    "# Load pulse timestamp file and unit timestamp file\n",
    "excel_folder = r\"C:\\StimTimestamps\" \n",
    "csv_folder = r\"C:\\UnitTimeseries\"  \n",
    "output_folder = r\"C:\\Stim-Unit_Timestamps\"  \n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for excel_file in os.listdir(excel_folder):\n",
    "    if excel_file.endswith(\".xlsx\"):\n",
    "        excel_path = os.path.join(excel_folder, excel_file)\n",
    "        extracted_data = extract_data_from_excel(excel_path)\n",
    "        add_sheet_names_to_columns(extracted_data)\n",
    "        \n",
    "        for csv_file in os.listdir(csv_folder):\n",
    "            if csv_file.endswith(\".csv\") and excel_file[:28] in csv_file:\n",
    "                csv_path = os.path.join(csv_folder, csv_file)\n",
    "                merged_data = merge_excel_and_csv(extracted_data, csv_path)\n",
    "                output_csv_file_name = os.path.splitext(csv_file)[0] + \"_merged.csv\"\n",
    "                output_csv_file_path = os.path.join(output_folder, output_csv_file_name)\n",
    "                save_to_csv(output_csv_file_path, merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) convert the timestapm data into seconds\n",
    "## 2) Adds 50ms or 0.05 seconds to the Stim_OFF time stamps --> the ON time will be ON + (50ms of OFF time)\n",
    "## 2) Concatinate all ON and OFF time stamp data into two columns \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def adjust_and_concatenate_columns(csv_file, sampling_frequency):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df / sampling_frequency\n",
    "    column_a_data = []\n",
    "    column_b_data = []\n",
    "    for i in range(0, 14, 2):\n",
    "        column_a_data.extend(df.iloc[:, i].dropna().values)\n",
    "        column_b_data.extend(df.iloc[:, i+1].dropna().values)\n",
    "    column_b_data = [x + 0.05 for x in column_b_data]\n",
    "    new_df = pd.DataFrame({\n",
    "        'Stim_ON': column_a_data,\n",
    "        'Stim_OFF': column_b_data\n",
    "    })\n",
    "    \n",
    "    remaining_columns = df.iloc[:, 14:]\n",
    "    max_rows = max(new_df.shape[0], remaining_columns.shape[0])\n",
    "    new_df = new_df.reindex(range(max_rows))\n",
    "    remaining_columns = remaining_columns.reindex(range(max_rows))\n",
    "    final_df = pd.concat([new_df, remaining_columns], axis=1)\n",
    "    return final_df\n",
    "\n",
    "def process_files_and_save_to_csv(csv_folder, output_folder, sampling_frequency):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    for csv_file in os.listdir(csv_folder):\n",
    "        if csv_file.endswith(\".csv\"):\n",
    "            csv_path = os.path.join(csv_folder, csv_file)\n",
    "            df_adjusted = adjust_and_concatenate_columns(csv_path, sampling_frequency)\n",
    "            output_file_name = os.path.splitext(csv_file)[0] + \"_stimDivSampFreq_ONOFF0.05.csv\"\n",
    "            output_file = os.path.join(output_folder, output_file_name)\n",
    "            df_adjusted.to_csv(output_file, index=False)\n",
    "\n",
    "csv_folder = r\"C:\\Vasc_Stim\"  \n",
    "output_folder = r\"C:\\Stim_SampFreqAdj\"  \n",
    "sampling_frequency = 32000  # Specify the sampling frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "source_folder = r'C:\\Stim_SampFreqAdj'\n",
    "output_excel_path = r'C:\\Stim_SamFreqAdj_Freqcat.xlsx'\n",
    "\n",
    "def generate_frequency_pulse_width():\n",
    "    frequency = []\n",
    "    pulse_width = []\n",
    "    frequency.extend([0.5] * 5)\n",
    "    frequency.extend([1] * 10)\n",
    "    frequency.extend([2] * 20)\n",
    "    frequency.extend([5] * 25)\n",
    "    pulse_width.extend([50] * 60)\n",
    "    return frequency, pulse_width\n",
    "\n",
    "def process_csv_files(source_folder, output_excel_path):\n",
    "    with pd.ExcelWriter(output_excel_path, engine='xlsxwriter') as writer:\n",
    "        for file_name in os.listdir(source_folder):\n",
    "            if file_name.endswith('.csv'):\n",
    "                file_path = os.path.join(source_folder, file_name)\n",
    "                df = pd.read_csv(file_path)\n",
    "                frequency, pulse_width = generate_frequency_pulse_width()\n",
    "                new_columns_df = pd.DataFrame({\n",
    "                    'Frequency': frequency,\n",
    "                    'Pulse_width': pulse_width\n",
    "                })\n",
    "                combined_df = pd.concat([new_columns_df, df], axis=1)\n",
    "                sheet_name = os.path.splitext(file_name)[0]  \n",
    "                combined_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "# Run the process\n",
    "process_csv_files(source_folder, output_excel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## N of APs per ON and OFF\n",
    "import pandas as pd\n",
    "\n",
    "input_excel_path = r'C:\\Stim_SamFreqAdj_Freqcat.xlsx'\n",
    "output_excel_path = r'C:\\ON-OFF.xlsx'\n",
    "\n",
    "def process_sheet(df):\n",
    "    frequency = df['Frequency']\n",
    "    pulse_width = df['Pulse_width']\n",
    "    stim_on = df['Stim_ON'].dropna().values\n",
    "    stim_off = df['Stim_OFF'].dropna().values\n",
    "    neuronal_data = df.iloc[:, 4:]\n",
    "    neuron_names = neuronal_data.columns\n",
    "    on_off_results = {neuron_name: [] for neuron_name in neuron_names}\n",
    "    all_stim_on = []\n",
    "    all_stim_off = []\n",
    "    for on, off in zip(stim_on, stim_off):\n",
    "        adjusted_off = off + 0.05 \n",
    "        all_stim_on.append(on)\n",
    "        all_stim_off.append(adjusted_off)\n",
    "        for neuron_name in neuron_names:\n",
    "            neuron_timestamps = neuronal_data[neuron_name].dropna().values\n",
    "            count_within = sum(on <= ts <= adjusted_off for ts in neuron_timestamps)\n",
    "            on_off_results[neuron_name].append(count_within)\n",
    "    \n",
    "    on_off_data = pd.DataFrame(on_off_results)\n",
    "    on_off_data.insert(0, 'Stim_ON', all_stim_on)\n",
    "    on_off_data.insert(1, 'Stim_OFF', all_stim_off)\n",
    "    on_off_data.insert(0, 'Pulse_width', pulse_width)\n",
    "    on_off_data.insert(0, 'Frequency', frequency)\n",
    "    \n",
    "    off_next_on_results = {neuron_name: [] for neuron_name in neuron_names}\n",
    "    all_stim_on_next = []\n",
    "    all_stim_off_current = []\n",
    "    for i in range(len(stim_off)):\n",
    "        adjusted_off = stim_off[i] + 0.05 \n",
    "        next_on_time = stim_on[i + 1] if (i + 1) < len(stim_on) else None\n",
    "        \n",
    "        if next_on_time:\n",
    "            all_stim_on_next.append(next_on_time)\n",
    "            all_stim_off_current.append(adjusted_off)\n",
    "            for neuron_name in neuron_names:\n",
    "                neuron_timestamps = neuronal_data[neuron_name].dropna().values\n",
    "                count_within = sum(adjusted_off <= ts <= next_on_time for ts in neuron_timestamps)\n",
    "                off_next_on_results[neuron_name].append(count_within)\n",
    "    \n",
    "    # Convert OFF-Next ON results to DataFrame\n",
    "    off_next_on_data = pd.DataFrame(off_next_on_results)\n",
    "    off_next_on_data.insert(0, 'Stim_ON', all_stim_on_next)\n",
    "    off_next_on_data.insert(1, 'Stim_OFF', all_stim_off_current)\n",
    "    off_next_on_data.insert(0, 'Pulse_width', pulse_width)\n",
    "    off_next_on_data.insert(0, 'Frequency', frequency)\n",
    "    \n",
    "    return on_off_data, off_next_on_data\n",
    "\n",
    "excel_data = pd.ExcelFile(input_excel_path)\n",
    "with pd.ExcelWriter(output_excel_path, engine='xlsxwriter') as writer:\n",
    "    for sheet_name in excel_data.sheet_names:\n",
    "        df = pd.read_excel(excel_data, sheet_name=sheet_name)\n",
    "        on_off_data, off_next_on_data = process_sheet(df)\n",
    "        on_off_data.to_excel(writer, sheet_name=f'{sheet_name}_ON', index=False)\n",
    "        off_next_on_data.to_excel(writer, sheet_name=f'{sheet_name}_OFF', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Freq per ON and OFF periods\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "input_excel_path = r'C:\\ON-OFF.xlsx'\n",
    "output_excel_path = r'C:\\ON-OFF_Freq.xlsx'\n",
    "\n",
    "def process_sheet(df):\n",
    "    frequency = df['Frequency']\n",
    "    pulse_width = df['Pulse_width']\n",
    "    stim_on = df['Stim_ON'].dropna().values\n",
    "    stim_off = df['Stim_OFF'].dropna().values\n",
    "    stim_delta = abs(stim_on - stim_off)\n",
    "    neuronal_data = df.iloc[:, 4:]\n",
    "    for i in range(len(neuronal_data.columns)):\n",
    "        neuronal_data.iloc[:, i] = neuronal_data.iloc[:, i] / stim_delta\n",
    "    processed_df = pd.concat([frequency, pulse_width, df[['Stim_ON', 'Stim_OFF']], neuronal_data], axis=1)\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "excel_data = pd.ExcelFile(input_excel_path)\n",
    "with pd.ExcelWriter(output_excel_path, engine='xlsxwriter') as writer:\n",
    "    for sheet_name in excel_data.sheet_names:\n",
    "        df = pd.read_excel(excel_data, sheet_name=sheet_name)\n",
    "        processed_df = process_sheet(df)\n",
    "        processed_df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Determine Responsive and non-responsive Neurons\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "input_excel_path = r'C:\\ON-OFF_Freq.xlsx'\n",
    "output_excel_path = r'C:\\Responding_norm.xlsx'\n",
    "\n",
    "def sd(data):\n",
    "    return np.std(data, ddof=1)\n",
    "\n",
    "def process_neurons(on_df, off_df):\n",
    "    frequencies = on_df['Frequency'].unique()\n",
    "    res_data = {'Frequency': frequencies}\n",
    "    non_res_data = {'Frequency': frequencies}\n",
    "\n",
    "    for neuron in on_df.columns[4:]:\n",
    "        on_means = on_df.groupby('Frequency')[neuron].mean()\n",
    "        off_means = off_df.groupby('Frequency')[neuron].mean()\n",
    "        off_sds = off_df.groupby('Frequency')[neuron].apply(sd)\n",
    "        off_means_replaced = off_means.replace(0, 0.1)\n",
    "        normalized_values = on_means / off_means_replaced\n",
    "        res_count = (on_means > off_means + (2 * off_sds)).sum()\n",
    "        is_res = res_count >= 2\n",
    "        if is_res:\n",
    "            res_data[neuron] = normalized_values\n",
    "        else:\n",
    "            non_res_data[neuron] = normalized_values\n",
    "    res_df = pd.DataFrame(res_data)\n",
    "    non_res_df = pd.DataFrame(non_res_data)\n",
    "    return res_df, non_res_df\n",
    "\n",
    "excel_data = pd.ExcelFile(input_excel_path)\n",
    "with pd.ExcelWriter(output_excel_path, engine='xlsxwriter') as writer:\n",
    "    for sheet_name in excel_data.sheet_names:\n",
    "        if sheet_name.endswith('_ON'):\n",
    "            base_name = sheet_name[:-3]\n",
    "            off_sheet_name = f'{base_name}_OFF'\n",
    "            if off_sheet_name in excel_data.sheet_names:\n",
    "                on_df = pd.read_excel(excel_data, sheet_name=sheet_name)\n",
    "                off_df = pd.read_excel(excel_data, sheet_name=off_sheet_name)\n",
    "                res_df, non_res_df = process_neurons(on_df, off_df)\n",
    "                res_sheet_name = f'{base_name}_Res'\n",
    "                non_res_sheet_name = f'{base_name}_Non_Res'\n",
    "                res_df.to_excel(writer, sheet_name=res_sheet_name, index=False)\n",
    "                non_res_df.to_excel(writer, sheet_name=non_res_sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Categorization of data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "input_excel_path = r'C:\\Responding_Norma.xlsx'\n",
    "output_excel_path = r'C:\\Responding_Summarized.xlsx'\n",
    "\n",
    "def sem(data):\n",
    "    return np.std(data, ddof=1) / np.sqrt(len(data))\n",
    "\n",
    "def process_sheet(sheet_name, sheet_df):\n",
    "    parts = sheet_name.split('_')\n",
    "    group = parts[1]  # Vasc or NonV\n",
    "    week = parts[0]  # w17, w20, or w23\n",
    "    neuron_type = parts[4]  # Res or Non_Res\n",
    "    pulse_intensity = parts[3]  # _02_, _03_, or _04_\n",
    "\n",
    "    output_data = {\n",
    "        'Group': [],\n",
    "        'Week': [],\n",
    "        'Neuron_Type': [],\n",
    "        'Pulse_Intensity': [],\n",
    "        'Sample_Name': [],\n",
    "        'Frequency': [],\n",
    "        'Average': [],\n",
    "        'Number': [],\n",
    "        'Max': [],\n",
    "        'Min': [],\n",
    "        'Median': [],\n",
    "        'SDV': [],\n",
    "        'SEM': []\n",
    "    }\n",
    "\n",
    "    for frequency in sheet_df['Frequency'].unique():\n",
    "        freq_df = sheet_df[sheet_df['Frequency'] == frequency]\n",
    "        neuron_data = freq_df.iloc[:, 1:].dropna(axis=1, how='all')\n",
    "        if neuron_data.shape[1] == 0:\n",
    "            avg = 0\n",
    "            n = 0\n",
    "            max_val = 0\n",
    "            min_val = 0\n",
    "            median = 0\n",
    "            sdv = 0\n",
    "            sem_val = 0\n",
    "        else:\n",
    "            avg = neuron_data.mean(axis=1).mean()\n",
    "            n = neuron_data.shape[1]\n",
    "            max_val = neuron_data.max(axis=1).max()\n",
    "            min_val = neuron_data.min(axis=1).min()\n",
    "            median = neuron_data.median(axis=1).median()\n",
    "            sdv = neuron_data.std(axis=1).mean()\n",
    "            sem_val = neuron_data.apply(sem, axis=1).mean()\n",
    "\n",
    "        output_data['Group'].append(group)\n",
    "        output_data['Week'].append(week)\n",
    "        output_data['Neuron_Type'].append(neuron_type)\n",
    "        output_data['Pulse_Intensity'].append(pulse_intensity)\n",
    "        output_data['Sample_Name'].append(sheet_name)\n",
    "        output_data['Frequency'].append(frequency)\n",
    "        output_data['Average'].append(avg)\n",
    "        output_data['Number'].append(n)\n",
    "        output_data['Max'].append(max_val)\n",
    "        output_data['Min'].append(min_val)\n",
    "        output_data['Median'].append(median)\n",
    "        output_data['SDV'].append(sdv)\n",
    "        output_data['SEM'].append(sem_val)\n",
    "\n",
    "    return pd.DataFrame(output_data)\n",
    "\n",
    "excel_data = pd.ExcelFile(input_excel_path)\n",
    "all_data = []\n",
    "for sheet_name in excel_data.sheet_names:\n",
    "    sheet_df = pd.read_excel(excel_data, sheet_name=sheet_name)\n",
    "    processed_df = process_sheet(sheet_name, sheet_df)\n",
    "    all_data.append(processed_df)\n",
    "summary_df = pd.concat(all_data, ignore_index=True)\n",
    "summary_df.to_excel(output_excel_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove outliers, plot, save for Prism analysis\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "colors = {\n",
    "    'Non_Vasc_NonRespond': 'gray',\n",
    "    'Vasc_NonRespond': 'lightcoral',\n",
    "    'Non_Vasc_Respond': '#B88B72',  # A color between gray (#808080) and orange (#FFA500)\n",
    "    'Vasc_Respond': '#D4A017'  # A color between lightcoral (#F08080) and orange (#FFA500)\n",
    "}\n",
    "\n",
    "file_path = r'C:\\Responding_Summarized.xlsx'\n",
    "xls = pd.ExcelFile(file_path)\n",
    "\n",
    "def remove_outliers(df, column):\n",
    "    def _remove_outliers(group):\n",
    "        Q1 = group[column].quantile(0.25)\n",
    "        Q3 = group[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        return group[~((group[column] < (Q1 - 3 * IQR)) | (group[column] > (Q3 + 5 * IQR)))]\n",
    "    return df.groupby(['Group', 'Week']).apply(_remove_outliers).reset_index(drop=True)\n",
    "\n",
    "def calculate_group_statistics(df):\n",
    "    group_stats = df.groupby(['Week', 'Group'])['Average'].agg(['mean', 'std', 'sem', 'max', 'min', 'count']).reset_index()\n",
    "    # Calculate number of data points per group\n",
    "    group_counts = df.groupby('Group')['Average'].count().reset_index()\n",
    "    group_stats['Count'] = group_counts['Average']\n",
    "    \n",
    "    return group_stats\n",
    "processed_data = {}\n",
    "variable_stats_raw = {}\n",
    "variable_stats_cleaned = {}\n",
    "for sheet_name in xls.sheet_names:\n",
    "    df = pd.read_excel(xls, sheet_name)\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['Week', 'Average'])\n",
    "    group_stats_raw = calculate_group_statistics(df)\n",
    "    variable_stats_raw[sheet_name] = group_stats_raw\n",
    "    # --------------------- Remove outliers --------------------\n",
    "    df_cleaned = remove_outliers(df, 'Average')\n",
    "    processed_data[sheet_name] = df_cleaned\n",
    "    # Save the cleaned data to a new Excel file\n",
    "    #cleaned_excel_path = f\"{output_directory}/{sheet_name}_OutliersRemoved.xlsx\"\n",
    "    #df_cleaned.to_excel(cleaned_excel_path, index=False)\n",
    "    # Calculate statistics per group\n",
    "    group_stats = calculate_group_statistics(df_cleaned)\n",
    "    variable_stats_cleaned[sheet_name] = group_stats\n",
    "    group_order = ['Non_Vasc_NonRespond', 'Vasc_NonRespond', 'Non_Vasc_Respond', 'Vasc_Respond']\n",
    "    # Plot using seaborn with custom colors\n",
    "    plt.figure(figsize=(7, 5))  # Increase figure size\n",
    "    ax=sns.boxplot(data=df_cleaned, x='Week', y='Average', hue='Group', \n",
    "                order=['w17', 'w20', 'w23'],\n",
    "                hue_order=group_order, palette=colors, showfliers=False, linewidth=0.5)  \n",
    "    sns.stripplot(data=df_cleaned, x='Week', y='Average', hue='Group', \n",
    "                  order=['w17', 'w20', 'w23'],\n",
    "                  hue_order=group_order, dodge=True, jitter=True, edgecolor='black', linewidth=0.5, palette=colors, \n",
    "                  alpha=0.5, size=2.5)  \n",
    "    means = df_cleaned.groupby(['Week', 'Group'])['Average'].mean().reset_index()\n",
    "    for group, color in colors.items():\n",
    "        plt.plot(means[means['Group'] == group]['Week'], means[means['Group'] == group]['Average'], \n",
    "                 color=color, linestyle='-', linewidth=1, marker='o', markersize=5, markeredgecolor='black',\n",
    "                 markeredgewidth=1,zorder=10 )  # Add markers\n",
    "    for group, color in colors.items():\n",
    "        sems = df_cleaned.groupby(['Week', 'Group'])['Average'].sem().reset_index()\n",
    "        group_sems = sems[sems['Group'] == group]\n",
    "        for dpi in ['w17', 'w20', 'w23']:\n",
    "            dpi_sems = group_sems[group_sems['Week'] == dpi]\n",
    "            plt.errorbar(dpi_sems['Week'], means[(means['Group'] == group) & (means['Week'] == dpi)]['Average'], \n",
    "                         yerr=dpi_sems['Average'], fmt='o', color=color, markersize=8, capsize=3, \n",
    "                         markeredgecolor='black', linewidth=1, zorder=10)  # Add error bars with black color and adjustable line width\n",
    "    plt.xticks(rotation=45, fontsize=18)\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylim(1e-2, 1e2)  \n",
    "    plt.legend(title='Group', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xlabel('Week', fontsize=16)  \n",
    "    plt.ylabel('Firing Frequency (Hz)', fontsize=18)  \n",
    "    plt.title(f'{sheet_name}', fontsize=18)  \n",
    "    plt.yticks(fontsize=16)  \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    #output_directory= r'C:\\.....'\n",
    "    # Save the plot as a PDF file\n",
    "    #pdf_filename = f\"{output_directory}/{sheet_name}_VascNonVasc_Weeks.pdf\"\n",
    "    #plt.savefig(pdf_filename, format='pdf')\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
